# LangChain을 활용한 RAG 구성

## 1. 기본 사용법

 - `패키지 설치`
```bash
%pip install -q python-dotenv langchain-openai langchain-upstage langchain-community
```
<br/>

 - `환경 변수 불러오기`
    - .env 파일에 등록된 환경 변수를 로드한다.
```python
# ChatGPT: OPENAI_API_KEY
# Upstage: UPSTAGE_API_KEY
from dotenv import load_dotenv

load_dotenv()
```
<br/>

 - `LLM 답변 생성`
```python
# Chat GPT
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model_name='gpt-4o-mini')
ai_message = llm.invoke("인프런에 어떤 강의가 있나요?")
ai_message.content


# Uptstage
from langchain_upstage import ChatUpstage
llm = ChatUpstage()
ai_message = llm.invoke("인프런에 어떤 강의가 있나요?")
ai_message.content


# Ollama
# 터미널에서 ollama pull gemma2 와 같이 원하는 모델을 가져와야 실행 가능
from langchain_community.chat_models import ChatOllama
llm = ChatOllama(model="gemma2")
ai_message = llm.invoke("인프런에 어떤 강의가 있나요?")
ai_message.content
```
<br/>

## 2. LangChain과 Chroma를 활용한 LAG 구성

### 2-1. OpenmAIEmbeddings

 - `패키지 설치`
```bash
%pip install python-dotenv langchain langchain-openai langchain-community langchain-text-splitters docx2txt langchain-chroma
```
<br/>

 - `벡터 DB에 데이터 저장 및 사용`
```python
# 1. RecursiveCharacterTextSplitter를 활용한 데이터 chunking
from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# chunk_size: split 된 chunk의 최대 크기
# chunk_overlap: 앞 뒤로 나뉘어진 chunk들이 얼마나 겹쳐도 되는지 지정
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=200,
)

loader = Docx2txtLoader('./tax.docx')
document_list = loader.load_and_split(text_splitter=text_splitter)


# 2. 임베딩 모델 정의
# OpenAI에서 제공하는 Embedding Model을 활용해서 `chunk`를 vector화
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings

load_dotenv()
embedding = OpenAIEmbeddings(model='text-embedding-3-large')


# 3. 벡터 DB(Chroma)에 데이터 저장
from langchain_chroma import Chroma

# 데이터를 처음 저장할 때 
# database = Chroma.from_documents(documents=document_list, embedding=embedding, collection_name='chroma-tax', persist_directory="./chroma")

# 이미 저장된 데이터를 사용할 때 
database = Chroma(collection_name='chroma-tax', persist_directory="./chroma", embedding_function=embedding)

query = '연봉 5천만원인 직장인의 소득세는 얼마인가요?'
retrieved_docs = database.similarity_search(query, k=3)
```
<br/>

 - `LLM + 벡터 DB 연동`
```python
# 1. LangChain PromptTemplate 불러오기
from langchain_openai import ChatOpenAI
from langchain import hub

llm = ChatOpenAI(model='gpt-4o')
prompt = hub.pull("rlm/rag-prompt")

# 2. 답변 생성
# RetrievalQA를 통해 LLM에 전달
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm, 
    retriever=database.as_retriever(),
    chain_type_kwargs={"prompt": prompt}
)
query = '연봉 5천만원인 직장인의 소득세는 얼마인가요?'
ai_message = qa_chain.invoke({"query": query})
```
<br/>

### 2-2. Upstage Embeddings

 - `패키지 설치`
```bash
%pip install python-dotenv langchain langchain-upstage langchain-community langchain-text-splitters docx2txt langchain-chroma
```
<br/>

 - `벡터 DB에 데이터 저장 및 사용`
    - 벡터 DB는 Chroma 사용
    - 임베딩 모델은 Upstage Embeddings 사용
```python
# 1. RecursiveCharacterTextSplitter를 활용한 데이터 chunking
from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=200,
)

loader = Docx2txtLoader('./tax.docx')
document_list = loader.load_and_split(text_splitter=text_splitter)


# 2. 임베딜 모델 정의
from dotenv import load_dotenv
from langchain_upstage import UpstageEmbeddings

load_dotenv()
embedding = UpstageEmbeddings(model="solar-embedding-1-large")


# 3. 벡터 DB(Chroma)에 데이터 저장
from langchain_chroma import Chroma

# 데이터를 처음 저장할 때 
database = Chroma.from_documents(documents=document_list, embedding=embedding, collection_name='chroma-tax', persist_directory="./chroma")
```
<br/>

### 2-3. LangChain 없이 RAG 구현

 - `패키지 설치`
```bash
%pip install python-docx python-dotenv tiktoken chromadb openai
```
<br/>

 - `벡터 DB에 데이터 저장 및 사용`
    - LangChain의 TextSplitter를 사용할 수 없기 때문에 python-docx와 tiktoken을 활용해서 chunk 생성
```python
# 1. 파일에서 데이터 읽기
from docx import Document

document = Document('./tax.docx')

full_text = ''
for index, paragraph in enumerate(document.paragraphs):
    full_text += f'{paragraph.text}\n'


# 2. 문자열 나누는 함수 정의
import tiktoken 

def split_text(full_text, chunk_size):
    encoder = tiktoken.encoding_for_model("gpt-4o")
    total_encoding = encoder.encode(full_text)
    total_token_count = len(total_encoding)
    text_list = []
    for i in range(0, total_token_count, chunk_size):
        chunk = total_encoding[i: i+chunk_size]
        decoded = encoder.decode(chunk)
        text_list.append(decoded)
    
    return text_list


# 3. Chroma DB를 직접 Client를 이용해서 사용
import chromadb

chroma_client = chromadb.Client()
collection_name = 'tax_collection'
tax_collection = chroma_client.create_collection(collection_name)


# 4. 임베딜 모델 정의
import os
from dotenv import load_dotenv
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction

load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')
openai_embedding = OpenAIEmbeddingFunction(api_key=openai_api_key, model_name='text-embedding-3-large')


# 5. 벡터 DB에 데이터 저장
tax_collection = chroma_client.get_or_create_collection(collection_name, embedding_function=openai_embedding)

id_list = []
for index in range(len(chunk_list)):
    id_list.append(f'{index}')

tax_collection.add(documents=chunk_list, ids=id_list)


# 6. 답변 생성
# Chroma에 저장한 데이터를 유사도 검색(query)를 활용해서 가져옴
query = '연봉 5천만원인 직장인의 소득세는 얼마인가요?'
retrieved_doc = tax_collection.query(query_texts=query, n_results=3)
retrieved_doc['documents'][0]

from openai import OpenAI
client = OpenAI()

# Retrieval된 데이터는 system prompt에 추가해서 LLM의 배경지식으로 활용
response = client.chat.completions.create(
  model="gpt-4o",
  messages=[
    {"role": "system", "content": f"당신은 한국의 소득세 전문가 입니다. 아래 내용을 참고해서 사용자의 질문에 답변해주세요 {retrieved_doc['documents'][0]}"},
    {"role": "user", "content": query}
  ]
)
```
<br/>

## 3. 벡터 DB 변경

### 3-1. Pinecone

 - `패키지 설치`
```bash
%pip install langchain langchain-core langchain-community langchain-text-splitters langchain-openai langchain-pinecone docx2txt
```

 - `벡터 DB 생성`
    - RecursiveCharacterTextSplitter로 문서의 데이터를 분리하고 로드하는 작업은 생략
```python
import os

from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from pinecone import Pinecone
from langchain_pinecone import PineconeVectorStore

# 임베딜 모델
load_dotenv()
embedding = OpenAIEmbeddings(model='text-embedding-3-large')

# 데이터를 임베딩하여 Pinecon DB에 저장
index_name = 'tax-index'
pinecone_api_key = os.environ.get("PINECONE_API_KEY")
pc = Pinecone(api_key=pinecone_api_key)
database = PineconeVectorStore.from_documents(document_list, embedding, index_name=index_name)

# Retrieval 생성
retriever = database.as_retriever(search_kwargs={'k': 4})
```
<br/>

### 3-2. Pinecone DB + Upstage 임베딩

 - `패키지 설치`
```bash
%pip install python-dotenv langchain langchain-upstage langchain-community langchain-text-splitters langchain-pinecone docx2txt
```

 - ``
```python
from dotenv import load_dotenv
from langchain_upstage import UpstageEmbeddings
from langchain_pinecone import PineconeVectorStore

# 임베딜 모델
load_dotenv()
embedding = UpstageEmbeddings(model="solar-embedding-1-large")

# 데이터를 임베딩하여 Pinecon DB에 저장
index_name = 'tax-upstage-index'
database = PineconeVectorStore.from_documents(document_list, embedding, index_name=index_name)
```
<br/>

## 4. Retrieval 효율 개선을 위한 키워드 사전 활용

 - `패키지 설치`
```bash
%pip install langchain langchain-core langchain-community langchain-text-splitters langchain-openai langchain-pinecone
```

 - `데이터 준비`
```python
# 1. 문서에서 데이터 로드
from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=200,
)

loader = Docx2txtLoader('./tax_with_markdown.docx')
document_list = loader.load_and_split(text_splitter=text_splitter)

# 2. 임베딜 모델 정의
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings

load_dotenv()
embedding = OpenAIEmbeddings(model='text-embedding-3-large')

# 3. 벡터 DB 생성
import os
from pinecone import Pinecone
from langchain_pinecone import PineconeVectorStore

index_name = 'tax-markdown-index'
pinecone_api_key = os.environ.get("PINECONE_API_KEY")
pc = Pinecone(api_key=pinecone_api_key)
database = PineconeVectorStore.from_documents(document_list, embedding, index_name=index_name)
```

 - `답변 생성`
```python
# 1. Retrieval된 데이터는 LangChain에서 제공하는 프롬프트 사용
from langchain import hub

prompt = hub.pull("rlm/rag-prompt")


# 2. LLM 모델 정의
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model='gpt-4o')


# 3. LangChain 구성
from langchain.chains import RetrievalQA

# query -> retriever에서 문서 검색 -> 기존 query와 문서 내용이 참고된 프롬프트로 llm 요청
qa_chain = RetrievalQA.from_chain_type(
    llm, 
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt}
)
ai_message = qa_chain.invoke({"query": query})

# 4. keyword 사전 활용
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

dictionary = ["사람을 나타내는 표현 -> 거주자"]

prompt = ChatPromptTemplate.from_template(f"""
    사용자의 질문을 보고, 우리의 사전을 참고해서 사용자의 질문을 변경해주세요.
    만약 변경할 필요가 없다고 판단된다면, 사용자의 질문을 변경하지 않아도 됩니다.
    그런 경우에는 질문만 리턴해주세요
    사전: {dictionary}
    
    질문: {{question}}
""")

# prompt를 LLM에 요청하여 Keyword를 변경한 프롬프트로 변경
dictionary_chain = prompt | llm | StrOutputParser()

# 변경된 프롬프트로 qa_chain 요청
tax_chain = {"query": dictionary_chain} | qa_chain

ai_response = tax_chain.invoke({"question": query})
```