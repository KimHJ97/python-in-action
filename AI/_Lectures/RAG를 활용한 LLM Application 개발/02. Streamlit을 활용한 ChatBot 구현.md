# Streamlit을 활용한 ChatBot 구현

 - `requirements.txt`
```
aiohttp==3.9.5
aiosignal==1.3.1
altair==5.3.0
annotated-types==0.7.0
anyio==4.4.0
attrs==23.2.0
blinker==1.8.2
cachetools==5.3.3
certifi==2024.6.2
charset-normalizer==3.3.2
click==8.1.7
dataclasses-json==0.6.7
distro==1.9.0
frozenlist==1.4.1
gitdb==4.0.11
GitPython==3.1.43
h11==0.14.0
httpcore==1.0.5
httpx==0.27.0
idna==3.7
Jinja2==3.1.4
jsonpatch==1.33
jsonpointer==3.0.0
jsonschema==4.22.0
jsonschema-specifications==2023.12.1
langchain==0.2.3
langchain-community==0.2.4
langchain-core==0.2.5
langchain-openai==0.1.8
langchain-pinecone==0.1.1
langchain-text-splitters==0.2.1
langchainhub==0.1.20
langsmith==0.1.77
markdown-it-py==3.0.0
MarkupSafe==2.1.5
marshmallow==3.21.3
mdurl==0.1.2
multidict==6.0.5
mypy-extensions==1.0.0
numpy==1.26.4
openai==1.34.0
orjson==3.10.4
packaging==23.2
pandas==2.2.2
pillow==10.3.0
pinecone-client==3.2.2
protobuf==4.25.3
pyarrow==16.1.0
pydantic==2.7.4
pydantic_core==2.18.4
pydeck==0.9.1
Pygments==2.18.0
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
pytz==2024.1
PyYAML==6.0.1
referencing==0.35.1
regex==2024.5.15
requests==2.32.3
rich==13.7.1
rpds-py==0.18.1
six==1.16.0
smmap==5.0.1
sniffio==1.3.1
SQLAlchemy==2.0.30
streamlit==1.35.0
tenacity==8.3.0
tiktoken==0.7.0
toml==0.10.2
toolz==0.12.1
tornado==6.4.1
tqdm==4.66.4
types-requests==2.32.0.20240602
typing-inspect==0.9.0
typing_extensions==4.12.2
tzdata==2024.1
urllib3==2.2.1
yarl==1.9.4
```

 - `config.py`
```python
answer_examples = [
    {
        "input": "소득은 어떻게 구분되나요?", 
        "answer": """소득세법 제 4조(소득의 구분)에 따르면 소득은 아래와 같이 구분됩니다.
1. 종합소득
    - 이 법에 따라 과세되는 모든 소득에서 제2호 및 제3호에 따른 소득을 제외한 소득으로서 다음 각 목의 소득을 합산한 것
    - 가. 이자소득
    - 나. 배당소득
    - 다. 사업소득
    - 라. 근로소득
    - 마. 연금소득
    - 바. 기타소득
2. 퇴직소득
3. 양도소득
"""
    },
    {
        "input": "소득세의 과세 기간은 어떻게 되나요?", 
        "answer": """소득세법 제5조(과세기간)에 따르면, 
일반적인 소득세의 과세기간은 1월 1일부터 12월 31일까지 1년입니다
하지만 거주자가 사망한 경우는 1월 1일부터 사망일까지, 
거주자가 해외로 이주한 경우 1월 1일부터 출국한 날까지 입니다"""
    },
    {
        "input": "원천징수 영수증은 언제 발급받을 수 있나요?", 
        "answer": """소득세법 제143조(근로소득에 대한 원천징수영수증의 발급)에 따르면, 
근로소득을 지급하는 원천징수의무자는 해당 과세기간의 다음 연도 2월 말일까지 원천징수영수증을 근로소득자에게 발급해야하고. 
다만, 해당 과세기간 중도에 퇴직한 사람에게는 퇴직한 한 날의 다음 달 말일까지 발급하여야 하며, 
일용근로자에 대하여는 근로소득의 지급일이 속하는 달의 다음 달 말일까지 발급하여야 합니다.
만약 퇴사자가 원청징수영수증을 요청한다면 지체없이 바로 발급해야 합니다"""
    },
]
```

 - `llm.py`
```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from config import answer_examples  # 사전 정의된 few-shot 예시 불러오기

# 세션별 채팅 히스토리를 저장하는 메모리 구조
store = {}

# 세션 ID를 기반으로 채팅 히스토리를 가져오거나 생성
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# 벡터 검색을 위한 Pinecone 기반 retriever 생성
def get_retriever():
    embedding = OpenAIEmbeddings(model='text-embedding-3-large')  # OpenAI 임베딩 모델
    index_name = 'tax-markdown-index'  # Pinecone에 구축된 인덱스명
    database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)
    retriever = database.as_retriever(search_kwargs={'k': 4})  # 가장 유사한 문서 4개 검색
    return retriever

# 히스토리 기반 retriever 생성 (질문을 대화형 컨텍스트로부터 standalone 질문으로 변환)
def get_history_retriever():
    llm = get_llm()
    retriever = get_retriever()

    # 질문 리포맷용 시스템 프롬프트 정의
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    # 위 프롬프트로 ChatPromptTemplate 생성
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    # 히스토리 인식 retriever 생성
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    return history_aware_retriever

# OpenAI 기반 LLM 생성 함수 (기본 모델: gpt-4o)
def get_llm(model='gpt-4o'):
    return ChatOpenAI(model=model)

# 질문을 사전을 참고하여 바꿔주는 체인 생성 (Preprocessing)
def get_dictionary_chain():
    dictionary = ["사람을 나타내는 표현 -> 거주자"]  # 도메인 특화 사전
    llm = get_llm()

    prompt = ChatPromptTemplate.from_template(f"""
        사용자의 질문을 보고, 우리의 사전을 참고해서 사용자의 질문을 변경해주세요.
        만약 변경할 필요가 없다고 판단된다면, 사용자의 질문을 변경하지 않아도 됩니다.
        그런 경우에는 질문만 리턴해주세요
        사전: {dictionary}
        
        질문: {{question}}
    """)

    # 프롬프트 → LLM → 문자열 파싱으로 연결된 체인 구성
    dictionary_chain = prompt | llm | StrOutputParser()
    return dictionary_chain

# 전체 RAG(Retrieval-Augmented Generation) 체인 생성
def get_rag_chain():
    llm = get_llm()

    # Few-shot 예시용 프롬프트 템플릿 정의
    example_prompt = ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{answer}"),
        ]
    )

    # 실제 few-shot 예시 바인딩
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=answer_examples,
    )

    # 시스템 프롬프트 정의: 문서를 활용한 소득세법 QA
    system_prompt = (
        "당신은 소득세법 전문가입니다. 사용자의 소득세법에 관한 질문에 답변해주세요"
        "아래에 제공된 문서를 활용해서 답변해주시고"
        "답변을 알 수 없다면 모른다고 답변해주세요"
        "답변을 제공할 때는 소득세법 (XX조)에 따르면 이라고 시작하면서 답변해주시고"
        "2-3 문장정도의 짧은 내용의 답변을 원합니다"
        "\n\n"
        "{context}"
    )

    # 전체 QA 프롬프트 정의
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            few_shot_prompt,
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    # 히스토리 인식 retriever 및 문서 기반 QA 체인 구성
    history_aware_retriever = get_history_retriever()
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    # 최종 RAG 체인 구성
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    # 대화 히스토리와 함께 작동하도록 래핑
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,               # 세션 ID 기반 채팅 히스토리 관리
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    ).pick('answer')  # 결과에서 answer만 추출

    return conversational_rag_chain

# 최종 응답 생성 함수 (입력 → 사전 체인 → RAG 체인 → 출력)
def get_ai_response(user_message):
    dictionary_chain = get_dictionary_chain()
    rag_chain = get_rag_chain()

    # 사전 체인을 거쳐서 질문 전처리 후 RAG 실행
    tax_chain = {"input": dictionary_chain} | rag_chain

    # 세션 ID 설정 및 입력 전달 → 스트리밍 응답 생성
    ai_response = tax_chain.stream(
        {
            "question": user_message
        },
        config={
            "configurable": {"session_id": "abc123"}  # 임시 세션 ID
        },
    )

    return ai_response
```

 - `chat.py`
```python
import streamlit as st
from dotenv import load_dotenv
from llm import get_ai_response

st.set_page_config(page_title="소득세 챗봇", page_icon="🤖")
st.title("🤖 소득세 챗봇")
st.caption("소득세에 관련된 모든것을 답해드립니다!")

load_dotenv()

if 'message_list' not in st.session_state:
    st.session_state.message_list = []

for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])


if user_question := st.chat_input(placeholder="소득세에 관련된 궁금한 내용들을 말씀해주세요!"):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role": "user", "content": user_question})

    with st.spinner("답변을 생성하는 중입니다"):
        ai_response = get_ai_response(user_question)
        with st.chat_message("ai"):
            ai_message = st.write_stream(ai_response)
            st.session_state.message_list.append({"role": "ai", "content": ai_message})
```
