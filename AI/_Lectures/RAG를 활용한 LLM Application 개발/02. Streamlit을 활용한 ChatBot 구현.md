# Streamlitì„ í™œìš©í•œ ChatBot êµ¬í˜„

 - `requirements.txt`
```
aiohttp==3.9.5
aiosignal==1.3.1
altair==5.3.0
annotated-types==0.7.0
anyio==4.4.0
attrs==23.2.0
blinker==1.8.2
cachetools==5.3.3
certifi==2024.6.2
charset-normalizer==3.3.2
click==8.1.7
dataclasses-json==0.6.7
distro==1.9.0
frozenlist==1.4.1
gitdb==4.0.11
GitPython==3.1.43
h11==0.14.0
httpcore==1.0.5
httpx==0.27.0
idna==3.7
Jinja2==3.1.4
jsonpatch==1.33
jsonpointer==3.0.0
jsonschema==4.22.0
jsonschema-specifications==2023.12.1
langchain==0.2.3
langchain-community==0.2.4
langchain-core==0.2.5
langchain-openai==0.1.8
langchain-pinecone==0.1.1
langchain-text-splitters==0.2.1
langchainhub==0.1.20
langsmith==0.1.77
markdown-it-py==3.0.0
MarkupSafe==2.1.5
marshmallow==3.21.3
mdurl==0.1.2
multidict==6.0.5
mypy-extensions==1.0.0
numpy==1.26.4
openai==1.34.0
orjson==3.10.4
packaging==23.2
pandas==2.2.2
pillow==10.3.0
pinecone-client==3.2.2
protobuf==4.25.3
pyarrow==16.1.0
pydantic==2.7.4
pydantic_core==2.18.4
pydeck==0.9.1
Pygments==2.18.0
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
pytz==2024.1
PyYAML==6.0.1
referencing==0.35.1
regex==2024.5.15
requests==2.32.3
rich==13.7.1
rpds-py==0.18.1
six==1.16.0
smmap==5.0.1
sniffio==1.3.1
SQLAlchemy==2.0.30
streamlit==1.35.0
tenacity==8.3.0
tiktoken==0.7.0
toml==0.10.2
toolz==0.12.1
tornado==6.4.1
tqdm==4.66.4
types-requests==2.32.0.20240602
typing-inspect==0.9.0
typing_extensions==4.12.2
tzdata==2024.1
urllib3==2.2.1
yarl==1.9.4
```

 - `config.py`
```python
answer_examples = [
    {
        "input": "ì†Œë“ì€ ì–´ë–»ê²Œ êµ¬ë¶„ë˜ë‚˜ìš”?", 
        "answer": """ì†Œë“ì„¸ë²• ì œ 4ì¡°(ì†Œë“ì˜ êµ¬ë¶„)ì— ë”°ë¥´ë©´ ì†Œë“ì€ ì•„ë˜ì™€ ê°™ì´ êµ¬ë¶„ë©ë‹ˆë‹¤.
1. ì¢…í•©ì†Œë“
    - ì´ ë²•ì— ë”°ë¼ ê³¼ì„¸ë˜ëŠ” ëª¨ë“  ì†Œë“ì—ì„œ ì œ2í˜¸ ë° ì œ3í˜¸ì— ë”°ë¥¸ ì†Œë“ì„ ì œì™¸í•œ ì†Œë“ìœ¼ë¡œì„œ ë‹¤ìŒ ê° ëª©ì˜ ì†Œë“ì„ í•©ì‚°í•œ ê²ƒ
    - ê°€. ì´ìì†Œë“
    - ë‚˜. ë°°ë‹¹ì†Œë“
    - ë‹¤. ì‚¬ì—…ì†Œë“
    - ë¼. ê·¼ë¡œì†Œë“
    - ë§ˆ. ì—°ê¸ˆì†Œë“
    - ë°”. ê¸°íƒ€ì†Œë“
2. í‡´ì§ì†Œë“
3. ì–‘ë„ì†Œë“
"""
    },
    {
        "input": "ì†Œë“ì„¸ì˜ ê³¼ì„¸ ê¸°ê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", 
        "answer": """ì†Œë“ì„¸ë²• ì œ5ì¡°(ê³¼ì„¸ê¸°ê°„)ì— ë”°ë¥´ë©´, 
ì¼ë°˜ì ì¸ ì†Œë“ì„¸ì˜ ê³¼ì„¸ê¸°ê°„ì€ 1ì›” 1ì¼ë¶€í„° 12ì›” 31ì¼ê¹Œì§€ 1ë…„ì…ë‹ˆë‹¤
í•˜ì§€ë§Œ ê±°ì£¼ìê°€ ì‚¬ë§í•œ ê²½ìš°ëŠ” 1ì›” 1ì¼ë¶€í„° ì‚¬ë§ì¼ê¹Œì§€, 
ê±°ì£¼ìê°€ í•´ì™¸ë¡œ ì´ì£¼í•œ ê²½ìš° 1ì›” 1ì¼ë¶€í„° ì¶œêµ­í•œ ë‚ ê¹Œì§€ ì…ë‹ˆë‹¤"""
    },
    {
        "input": "ì›ì²œì§•ìˆ˜ ì˜ìˆ˜ì¦ì€ ì–¸ì œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?", 
        "answer": """ì†Œë“ì„¸ë²• ì œ143ì¡°(ê·¼ë¡œì†Œë“ì— ëŒ€í•œ ì›ì²œì§•ìˆ˜ì˜ìˆ˜ì¦ì˜ ë°œê¸‰)ì— ë”°ë¥´ë©´, 
ê·¼ë¡œì†Œë“ì„ ì§€ê¸‰í•˜ëŠ” ì›ì²œì§•ìˆ˜ì˜ë¬´ìëŠ” í•´ë‹¹ ê³¼ì„¸ê¸°ê°„ì˜ ë‹¤ìŒ ì—°ë„ 2ì›” ë§ì¼ê¹Œì§€ ì›ì²œì§•ìˆ˜ì˜ìˆ˜ì¦ì„ ê·¼ë¡œì†Œë“ìì—ê²Œ ë°œê¸‰í•´ì•¼í•˜ê³ . 
ë‹¤ë§Œ, í•´ë‹¹ ê³¼ì„¸ê¸°ê°„ ì¤‘ë„ì— í‡´ì§í•œ ì‚¬ëŒì—ê²ŒëŠ” í‡´ì§í•œ í•œ ë‚ ì˜ ë‹¤ìŒ ë‹¬ ë§ì¼ê¹Œì§€ ë°œê¸‰í•˜ì—¬ì•¼ í•˜ë©°, 
ì¼ìš©ê·¼ë¡œìì— ëŒ€í•˜ì—¬ëŠ” ê·¼ë¡œì†Œë“ì˜ ì§€ê¸‰ì¼ì´ ì†í•˜ëŠ” ë‹¬ì˜ ë‹¤ìŒ ë‹¬ ë§ì¼ê¹Œì§€ ë°œê¸‰í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
ë§Œì•½ í‡´ì‚¬ìê°€ ì›ì²­ì§•ìˆ˜ì˜ìˆ˜ì¦ì„ ìš”ì²­í•œë‹¤ë©´ ì§€ì²´ì—†ì´ ë°”ë¡œ ë°œê¸‰í•´ì•¼ í•©ë‹ˆë‹¤"""
    },
]
```

 - `llm.py`
```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from config import answer_examples  # ì‚¬ì „ ì •ì˜ëœ few-shot ì˜ˆì‹œ ë¶ˆëŸ¬ì˜¤ê¸°

# ì„¸ì…˜ë³„ ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì €ì¥í•˜ëŠ” ë©”ëª¨ë¦¬ êµ¬ì¡°
store = {}

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ Pinecone ê¸°ë°˜ retriever ìƒì„±
def get_retriever():
    embedding = OpenAIEmbeddings(model='text-embedding-3-large')  # OpenAI ì„ë² ë”© ëª¨ë¸
    index_name = 'tax-markdown-index'  # Pineconeì— êµ¬ì¶•ëœ ì¸ë±ìŠ¤ëª…
    database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)
    retriever = database.as_retriever(search_kwargs={'k': 4})  # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 4ê°œ ê²€ìƒ‰
    return retriever

# íˆìŠ¤í† ë¦¬ ê¸°ë°˜ retriever ìƒì„± (ì§ˆë¬¸ì„ ëŒ€í™”í˜• ì»¨í…ìŠ¤íŠ¸ë¡œë¶€í„° standalone ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜)
def get_history_retriever():
    llm = get_llm()
    retriever = get_retriever()

    # ì§ˆë¬¸ ë¦¬í¬ë§·ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    # ìœ„ í”„ë¡¬í”„íŠ¸ë¡œ ChatPromptTemplate ìƒì„±
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    # íˆìŠ¤í† ë¦¬ ì¸ì‹ retriever ìƒì„±
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    return history_aware_retriever

# OpenAI ê¸°ë°˜ LLM ìƒì„± í•¨ìˆ˜ (ê¸°ë³¸ ëª¨ë¸: gpt-4o)
def get_llm(model='gpt-4o'):
    return ChatOpenAI(model=model)

# ì§ˆë¬¸ì„ ì‚¬ì „ì„ ì°¸ê³ í•˜ì—¬ ë°”ê¿”ì£¼ëŠ” ì²´ì¸ ìƒì„± (Preprocessing)
def get_dictionary_chain():
    dictionary = ["ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì"]  # ë„ë©”ì¸ íŠ¹í™” ì‚¬ì „
    llm = get_llm()

    prompt = ChatPromptTemplate.from_template(f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
        ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”
        ì‚¬ì „: {dictionary}
        
        ì§ˆë¬¸: {{question}}
    """)

    # í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ ë¬¸ìì—´ íŒŒì‹±ìœ¼ë¡œ ì—°ê²°ëœ ì²´ì¸ êµ¬ì„±
    dictionary_chain = prompt | llm | StrOutputParser()
    return dictionary_chain

# ì „ì²´ RAG(Retrieval-Augmented Generation) ì²´ì¸ ìƒì„±
def get_rag_chain():
    llm = get_llm()

    # Few-shot ì˜ˆì‹œìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    example_prompt = ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{answer}"),
        ]
    )

    # ì‹¤ì œ few-shot ì˜ˆì‹œ ë°”ì¸ë”©
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=answer_examples,
    )

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜: ë¬¸ì„œë¥¼ í™œìš©í•œ ì†Œë“ì„¸ë²• QA
    system_prompt = (
        "ë‹¹ì‹ ì€ ì†Œë“ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì†Œë“ì„¸ë²•ì— ê´€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”"
        "ì•„ë˜ì— ì œê³µëœ ë¬¸ì„œë¥¼ í™œìš©í•´ì„œ ë‹µë³€í•´ì£¼ì‹œê³ "
        "ë‹µë³€ì„ ì•Œ ìˆ˜ ì—†ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ì£¼ì„¸ìš”"
        "ë‹µë³€ì„ ì œê³µí•  ë•ŒëŠ” ì†Œë“ì„¸ë²• (XXì¡°)ì— ë”°ë¥´ë©´ ì´ë¼ê³  ì‹œì‘í•˜ë©´ì„œ ë‹µë³€í•´ì£¼ì‹œê³ "
        "2-3 ë¬¸ì¥ì •ë„ì˜ ì§§ì€ ë‚´ìš©ì˜ ë‹µë³€ì„ ì›í•©ë‹ˆë‹¤"
        "\n\n"
        "{context}"
    )

    # ì „ì²´ QA í”„ë¡¬í”„íŠ¸ ì •ì˜
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            few_shot_prompt,
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    # íˆìŠ¤í† ë¦¬ ì¸ì‹ retriever ë° ë¬¸ì„œ ê¸°ë°˜ QA ì²´ì¸ êµ¬ì„±
    history_aware_retriever = get_history_retriever()
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    # ìµœì¢… RAG ì²´ì¸ êµ¬ì„±
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜ ì‘ë™í•˜ë„ë¡ ë˜í•‘
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,               # ì„¸ì…˜ ID ê¸°ë°˜ ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    ).pick('answer')  # ê²°ê³¼ì—ì„œ answerë§Œ ì¶”ì¶œ

    return conversational_rag_chain

# ìµœì¢… ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (ì…ë ¥ â†’ ì‚¬ì „ ì²´ì¸ â†’ RAG ì²´ì¸ â†’ ì¶œë ¥)
def get_ai_response(user_message):
    dictionary_chain = get_dictionary_chain()
    rag_chain = get_rag_chain()

    # ì‚¬ì „ ì²´ì¸ì„ ê±°ì³ì„œ ì§ˆë¬¸ ì „ì²˜ë¦¬ í›„ RAG ì‹¤í–‰
    tax_chain = {"input": dictionary_chain} | rag_chain

    # ì„¸ì…˜ ID ì„¤ì • ë° ì…ë ¥ ì „ë‹¬ â†’ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
    ai_response = tax_chain.stream(
        {
            "question": user_message
        },
        config={
            "configurable": {"session_id": "abc123"}  # ì„ì‹œ ì„¸ì…˜ ID
        },
    )

    return ai_response
```

 - `chat.py`
```python
import streamlit as st
from dotenv import load_dotenv
from llm import get_ai_response

st.set_page_config(page_title="ì†Œë“ì„¸ ì±—ë´‡", page_icon="ğŸ¤–")
st.title("ğŸ¤– ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ì— ê´€ë ¨ëœ ëª¨ë“ ê²ƒì„ ë‹µí•´ë“œë¦½ë‹ˆë‹¤!")

load_dotenv()

if 'message_list' not in st.session_state:
    st.session_state.message_list = []

for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])


if user_question := st.chat_input(placeholder="ì†Œë“ì„¸ì— ê´€ë ¨ëœ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”!"):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role": "user", "content": user_question})

    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤"):
        ai_response = get_ai_response(user_question)
        with st.chat_message("ai"):
            ai_message = st.write_stream(ai_response)
            st.session_state.message_list.append({"role": "ai", "content": ai_message})
```
