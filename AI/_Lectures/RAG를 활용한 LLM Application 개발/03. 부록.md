# 1. LangSmith

LangSmith는 LangChain 팀에서 만든 도구로, LLM 기반 애플리케이션을 디버깅, 평가, 관찰(Observability) 할 수 있게 도와주는 LLM 개발/운영 플랫폼이다.
 - 목적: LLM 앱의 추론 과정, 입출력, Chain/Agent 동작을 추적 및 분석
 - 기능: 디버깅, 로깅, 테스트, 평가, 데이터 집합 관리
 - 관련 도구: LangChain, LangGraph 등과 연동

## LLM Evaluation

LLM Evaluation은 대형 언어 모델(Large Language Model, LLM)의 성능이나 품질을 측정하고 평가하는 과정을 의미한다. 쉽게, GPT 같은 모델이 얼마나 똑똑한지, 질문에 얼마나 잘 대답하는지, 논리적으로 맞는 말을 하는지 등을 평가한다.

### 평가 종류

 - `정량 평가 (Automatic Evaluation)`
    - 컴퓨터가 자동으로 점수를 매기는 방식
    - __BLEU / ROUGE / METEOR__
        - → 번역 품질을 비교할 때 사용
        - → 정답과 얼마나 유사한지를 숫자로 평가
    - __Accuracy / F1 Score__
        - → 정답을 맞췄는지를 판단
        - → 예: 퀴즈 문제에서 답이 맞았는가?
    - __Log-likelihood / Perplexity__
        - → 모델이 문장을 예측할 때 얼마나 확신했는지 측정
 - `정성 평가 (Human Evaluation)`
    - 사람이 직접 보고 평가하는 방식
    - __정확성 (Correctness)__
        - → 답이 사실인지, 거짓이 없는지
    - __일관성 (Consistency)__
        - → 문장 내 논리가 맞는지, 앞뒤 말이 충돌하지 않는지
    - __창의성 (Creativity)__
        - → 단순한 답이 아니라 새롭고 유용한 방식으로 표현됐는지
    - __유용성 (Helpfulness)__
        - → 실제로 사람에게 도움이 되는 대답인지
 - `Task-based Evaluation`
    - 실제 과제를 주고 얼마나 잘 수행하는지 평가
    - 예시:
        - 문서 요약: 주어진 문서를 요약하라고 했을 때 얼마나 잘 했는가?
        - 질의응답: 질문에 대해 얼마나 정확하게 답하는가?
        - 코드 생성: 주어진 설명대로 코드를 잘 짰는가?

### 예제

LangSmith + LangChain + Pinecone + OpenAI를 이용해 한국 소득세법 기반 RAG 시스템을 평가

 - `평가용 Dataset 생성`
    - 입력 (input_question): 사용자의 질문
    - 출력 (output_answer): 정답 역할을 하는 기준 답변
    - 메타데이터 (contexts): 문서(RAG의 grounding 정보)
```python
from langsmith import Client
client = Client()

dataset = client.create_dataset("income_tax_dataset")
client.create_examples(inputs=[...], outputs=[...], metadata=[...], dataset_id=dataset.id)
```


 - `Pinecone에서 문서 검색용 Retriever 생성`
```python
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

embedding = OpenAIEmbeddings(model='text-embedding-3-large')
database = PineconeVectorStore.from_existing_index(index_name='tax-markdown-index', embedding=embedding)
retriever = database.as_retriever()
```

 - `RAG Bot 정의`
```python
import openai
from langsmith import traceable
from langsmith.wrappers import wrap_openai

class RagBot:

    def __init__(self, retriever, model: str = "gpt-4o"):
        self._retriever = retriever
        self._client = wrap_openai(openai.Client())
        self._model = model

    @traceable()
    def retrieve_docs(self, question):
        return self._retriever.invoke(question)

    @traceable()
    def invoke_llm(self, question, docs):
        response = self._client.chat.completions.create(
            model=self._model,
            messages=[
                {
                    "role": "system",
                    "content": "당신은 한국의 소득세 전문가입니다."
                    "아래 소득세법을 참고해서 사용자의 질문에 답변해주세요.\n\n"
                    f"## 소득세법\n\n{docs}",
                },
                {"role": "user", "content": question},
            ],
        )

        # Evaluators 를 활용해서 `answer`와 `context`를 평가할 예정
        return {
            "answer": response.choices[0].message.content,
            "contexts": [str(doc) for doc in docs],
        }

    @traceable()
    def get_answer(self, question):
        docs = self.retrieve_docs(question)
        return self.invoke_llm(question, docs)
```


 - `평가 대상 예측기 정의`
```python
def predict_rag_answer(example: dict):
    """답변만 평가할 때 사용"""
    response = rag_bot.get_answer(example["input_question"])
    return {"answer": response["answer"]}

def predict_rag_answer_with_context(example: dict):
    """Context를 활용해서 hallucination을 평가할 때 사용"""
    response = rag_bot.get_answer(example["input_question"])
    return {"answer": response["answer"], "contexts": response["contexts"]}
```

 - `평가 프롬프트 불러오기`
```python
from langchain import hub
from langchain_openai import ChatOpenAI

# Grade prompt
# 답변의 정확도를 측정하기위해 사용되는 프롬프트
grade_prompt_answer_accuracy = prompt = hub.pull("langchain-ai/rag-answer-vs-reference")

def answer_evaluator(run, example) -> dict:
    input_question = example.inputs["input_question"]
    reference = example.outputs["output_answer"]
    prediction = run.outputs["answer"]

    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    answer_grader = grade_prompt_answer_accuracy | llm

    score = answer_grader.invoke({"question": input_question,
                                  "correct_answer": reference,
                                  "student_answer": prediction})
    score = score["Score"]

    return {"key": "answer_v_reference_score", "score": score}


# Grade prompt
# 답변이 사용자의 질문에 얼마나 도움되는지 판단하는 프롬프트
grade_prompt_answer_helpfulness = prompt = hub.pull("langchain-ai/rag-answer-helpfulness")

def answer_helpfulness_evaluator(run, example) -> dict:
    input_question = example.inputs["input_question"]
    prediction = run.outputs["answer"]

    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    answer_grader = grade_prompt_answer_helpfulness | llm

    score = answer_grader.invoke({"question": input_question,
                                  "student_answer": prediction})
    score = score["Score"]

    return {"key": "answer_helpfulness_score", "score": score}


# Prompt
# hallucination 판단을 위한 프롬프트
grade_prompt_hallucinations = prompt = hub.pull("langchain-ai/rag-answer-hallucination")

def answer_hallucination_evaluator(run, example) -> dict:
    input_question = example.inputs["input_question"]
    contexts = run.outputs["contexts"]

    prediction = run.outputs["answer"]

    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    answer_grader = grade_prompt_hallucinations | llm

    score = answer_grader.invoke({"documents": contexts,
                                  "student_answer": prediction})
    score = score["Score"]

    return {"key": "answer_hallucination", "score": score}
```
<br/>

# 2. Hugging Face 오픈소스 LLM 활용방법

 - `패키지 설치`
```bash
!pip install -q langchain transformers langchain-huggingface langchain-community langchain-core langchain-text-splitters bitsandbytes docx2txt langchain-chroma
```

 - `모델 불러오기 & 사용`
```python
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline

# 모델 불러오기
llm = HuggingFacePipeline.from_model_id(
    model_id="microsoft/Phi-3-mini-4k-instruct",
    task="text-generation",
    pipeline_kwargs=dict(
        max_new_tokens=512,
        do_sample=False,
        repetition_penalty=1.03,
    ),
)
chat_model = ChatHuggingFace(llm=llm)

# 답변 생성하기
ai_message = chat_model.invoke("what is huggingface?")
```

 - `모델 양자화`
    - 양자화(Quantization)는 모델을 더 작고 빠르게 실행 가능하게 만드는 기술
```python
from transformers import BitsAndBytesConfig

# 모델 불러오기
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True,
)
quantized_llm = HuggingFacePipeline.from_model_id(
    model_id="microsoft/Phi-3-mini-4k-instruct",
    task="text-generation",
    pipeline_kwargs=dict(
        max_new_tokens=512,
        do_sample=False,
        repetition_penalty=1.03,
    ),
    model_kwargs={"quantization_config": quantization_config},
)
quantized_chat_model = ChatHuggingFace(llm=quantized_llm)

# 답변 생성핳기
quantized_ai_message = quantized_chat_model.invoke("what is huggingface?")
```

 - `벡터 DB & 리트리버`
```python
from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain import hub

# 문자열 자르기
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=200,
)
loader = Docx2txtLoader('./tax_with_markdown.docx')
document_list = loader.load_and_split(text_splitter=text_splitter)

# 임베딩 모델 정의
embedding = HuggingFaceEmbeddings(model_name='snunlp/KR-SBERT-V40K-klueNLI-augSTS')

# 벡터 DB 생성
database = Chroma.from_documents(documents=document_list, embedding=embedding, collection_name='chroma-tax', persist_directory="./chroma_markdown")

# 리트리버 생성
retrieval_qa_chat_prompt = hub.pull("langchain-ai/retrieval-qa-chat")
retriever = database.as_retriever(search_kwargs={"k": 1})
combine_docs_chain = create_stuff_documents_chain(
    quantized_korean_llm, retrieval_qa_chat_prompt
)
retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)

# 답변 생성
rag_chain_message = retrieval_chain.invoke({"input": "연봉 5천만원인 직장인의 소득세는 얼마인가요?"})
```